{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Employee Churn\n",
    "In this notebook we are going to deal with employee data and develop a predictive model to analyze the employee tunrover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      "satisfaction            14999 non-null float64\n",
      "evaluation              14999 non-null float64\n",
      "number_of_projects      14999 non-null int64\n",
      "average_montly_hours    14999 non-null int64\n",
      "time_spend_company      14999 non-null int64\n",
      "work_accident           14999 non-null int64\n",
      "churn                   14999 non-null int64\n",
      "promotion               14999 non-null int64\n",
      "department              14999 non-null object\n",
      "salary                  14999 non-null object\n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the url and import the turnover data\n",
    "url = \"https://assets.datacamp.com/production/course_6221/datasets/turnover.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "As you can see we have two categorical variables, we want to know more about these.\n",
    "Two types of categorical variables:\n",
    "- Ordinal, can be ranked or ordered\n",
    "- Nominal, do not have an intrinsic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sales' 'accounting' 'hr' 'technical' 'support' 'management' 'IT'\n",
      " 'product_mng' 'marketing' 'RandD']\n",
      "['low' 'medium' 'high']\n"
     ]
    }
   ],
   "source": [
    "# Print the unique values of the \"department\" column\n",
    "print(data.department.unique())\n",
    "\n",
    "# Print the unique values of the \"salary\" column\n",
    "print(data.salary.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to encode the categories, so \"salary\" is ordered and optimized for the prediciton algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      "satisfaction            14999 non-null float64\n",
      "evaluation              14999 non-null float64\n",
      "number_of_projects      14999 non-null int64\n",
      "average_montly_hours    14999 non-null int64\n",
      "time_spend_company      14999 non-null int64\n",
      "work_accident           14999 non-null int64\n",
      "churn                   14999 non-null int64\n",
      "promotion               14999 non-null int64\n",
      "department              14999 non-null object\n",
      "salary                  14999 non-null int8\n",
      "dtypes: float64(2), int64(6), int8(1), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Change the type of the \"salary\" column to categorical\n",
    "data.salary = data.salary.astype('category')\n",
    "\n",
    "# Provide the correct order of categories\n",
    "data.salary = data.salary.cat.reorder_categories(['low', 'medium', 'high'])\n",
    "\n",
    "# Encode categories\n",
    "data.salary = data.salary.cat.codes\n",
    "\n",
    "# Check if \"salary\" is int8\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IT  RandD  accounting  hr  management  marketing  product_mng  sales  \\\n",
      "0   0      0           0   0           0          0            0      1   \n",
      "1   0      0           0   0           0          0            0      1   \n",
      "2   0      0           0   0           0          0            0      1   \n",
      "3   0      0           0   0           0          0            0      1   \n",
      "4   0      0           0   0           0          0            0      1   \n",
      "\n",
      "   support  technical  \n",
      "0        0          0  \n",
      "1        0          0  \n",
      "2        0          0  \n",
      "3        0          0  \n",
      "4        0          0  \n"
     ]
    }
   ],
   "source": [
    "# Get dummies and save them inside a new DataFrame\n",
    "departments = pd.get_dummies(data.department)\n",
    "\n",
    "# Take a quick look to the first 5 rows of departments\n",
    "print(departments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"accounting\" column to avoid \"dummy trap\"\n",
    "departments = departments.drop(\"accounting\", axis=1)\n",
    "\n",
    "# Drop the old column \"department\" as you don't need it anymore\n",
    "data = data.drop(\"department\", axis=1)\n",
    "\n",
    "# Join the new dataframe \"departments\" to your employee dataset: done\n",
    "data = data.join(departments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of Employees who churn\n",
    "If \"churn\" is 0, the employe is still at the company.\n",
    "\n",
    "If \"churn\" is 1, the employee has left the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    11428\n",
      "1     3571\n",
      "Name: churn, dtype: int64\n",
      "0    76.191746\n",
      "1    23.808254\n",
      "Name: churn, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Use len() function to get the total number of observations and save it as the number of employees\n",
    "n_employees = len(data)\n",
    "\n",
    "# Print the number of employees who left/stayed\n",
    "print(data.churn.value_counts())\n",
    "\n",
    "# Print the percentage of employees who left/stayed\n",
    "print(data.churn.value_counts()/n_employees*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "But first set the target and features.\n",
    "\n",
    "To make a prediction, we need to seperate two components:\n",
    "- dependent variables (target)\n",
    "- independent variables (features)\n",
    "\n",
    "After that, divide the data into train / test data with 4/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the dependent variable column (churn) and set it as target\n",
    "target = data.churn\n",
    "\n",
    "# Drop column churn and set everything else as features\n",
    "features = data.drop(\"churn\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for splitting dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use that function to create the splits both for target and for features\n",
    "# Set the test sample to be 25% of your observations\n",
    "target_train, target_test, features_train, features_test = train_test_split(target,features,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree classification\n",
    "Calculate the Gini index -> then we can decide on how to split the tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of people who stayed/left\n",
    "stayed = 37\n",
    "left = 1138\n",
    "\n",
    "#sum of stayed and left\n",
    "total = stayed + left\n",
    "\n",
    "#gini index\n",
    "gini = 2*(stayed/total)*(left/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split by B!\n"
     ]
    }
   ],
   "source": [
    "# Gini index in case of splitting by variable A or B\n",
    "gini_A = 0.65\n",
    "gini_B = 0.15\n",
    "\n",
    "# check which Gini is lower and use it for spliting\n",
    "if gini_A < gini_B:\n",
    "    print(\"split by A!\")\n",
    "else:\n",
    "    print(\"split by B!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the tree to data\n",
    "Develop the employee turnover prediction model using Decision Tree classification\n",
    "\n",
    "And fit the features to the model in the training set\n",
    "\n",
    "Then check its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the classification algorithm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize it and call model by specifying the random_state parameter\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Apply a decision tree model to fit features to the target\n",
    "model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.22666666666666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a decision tree model to fit features to the target in the training set\n",
    "model.fit(features_train,target_train)\n",
    "\n",
    "# Check the accuracy score of the prediction for the training set\n",
    "model.score(features_train,target_train)*100\n",
    "\n",
    "# Check the accuracy score of the prediction for the test set\n",
    "model.score(features_test,target_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It correctly predicted almost __98%__ of the test set right, now we want to visualize this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the graphical visualization export function\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Apply Decision Tree model to fit Features to the Target\n",
    "model.fit(features_train,target_train)\n",
    "\n",
    "# Export the tree to a dot file\n",
    "export_graphviz(model,\"tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ³ Decision Tree\n",
    "If you want to see the decision tree, you can go to this website: http://www.webgraphviz.com/ and paste the content of tree.dot file and preview it in your browser. Here you can see a small piece of the tree.\n",
    "\n",
    "![Log tree](https://assets.datacamp.com/production/repositories/1765/datasets/ea3fb776a7da174e4941fe6e22a4b76fe334fe37/tree_graph.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Evaluation the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now\n",
    "- Training accuracy = 100%\n",
    "- Testing accuracy = 97.23%\n",
    "\n",
    "This means overfitting (a common problem with decision trees), what can we do about it?\n",
    "- \"pruning the tree\", setting a limit on max depth\n",
    "- limited number of observations in one leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.71535247577563\n",
      "97.06666666666666\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DecisionTreeClassifier while limiting the depth of the tree to 5\n",
    "model_depth_5 = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model_depth_5.fit(features_train,target_train)\n",
    "\n",
    "# Print the accuracy of the prediction for the training set\n",
    "print(model_depth_5.score(features_train,target_train)*100)\n",
    "\n",
    "# Print the accuracy of the prediction for the test set\n",
    "print(model_depth_5.score(features_test,target_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.57747355320473\n",
      "96.13333333333334\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DecisionTreeClassifier while limiting the sample size in leaves to 100\n",
    "model_sample_100 = DecisionTreeClassifier(min_samples_leaf=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model_sample_100.fit(features_train,target_train)\n",
    "\n",
    "# Print the accuracy of the prediction (in percentage points) for the training set\n",
    "print(model_sample_100.score(features_train,target_train)*100)\n",
    "\n",
    "# Print the accuracy of the prediction (in percentage points) for the test set\n",
    "print(model_sample_100.score(features_test,target_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "#### Precision\n",
    "fraction of True Positives over the sum of True Positives and False _Positives_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9240641711229947"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the function to calculate precision score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Predict whether employees will churn using the test set\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "# Calculate precision score by comparing target_test with the prediction\n",
    "precision_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall score\n",
    "fraction of True Positives over the sum of True Positives and False _Negatives_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9632107023411371"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the function to calculate recall score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Use the initial model to predict churn\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "# Calculate recall score by comparing target_test with the prediction\n",
    "recall_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC / AUC score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691623087590718"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the function to calculate ROC/AUC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Use initial model to predict churn (based on features_test)\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "# Calculate ROC/AUC score by comparing target_test with the prediction\n",
    "roc_auc_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blancing classes\n",
    "We want to correct the model by solving the imbalance problem as you have seen with the difference between 'recall' and 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.70666666666668\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DecisionTreeClassifier \n",
    "model_depth_5_b = DecisionTreeClassifier(max_depth=5,class_weight=\"balanced\",random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model_depth_5_b.fit(features_train,target_train)\n",
    "\n",
    "# Print the accuracy of the prediction (in percentage points) for the test set\n",
    "print(model_depth_5_b.score(features_test,target_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Employee attrition models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9632107023411371\n",
      "0.9691623087590718\n",
      "0.9319955406911928\n",
      "0.959863876199084\n"
     ]
    }
   ],
   "source": [
    "# Print the recall score\n",
    "print(recall_score(target_test,prediction))\n",
    "# Print the ROC/AUC score\n",
    "print(roc_auc_score(target_test,prediction))\n",
    "\n",
    "# Initialize the model\n",
    "model_depth_7_b = DecisionTreeClassifier(max_depth=7,class_weight=\"balanced\",random_state=42)\n",
    "# Fit it to the training component\n",
    "model_depth_7_b.fit(features_train,target_train)\n",
    "# Make prediction using test component\n",
    "prediction_b = model_depth_7_b.predict(features_test)\n",
    "# Print the recall score for the balanced model\n",
    "print(recall_score(target_test, prediction_b))\n",
    "# Print the ROC/AUC score for the balanced model\n",
    "print(roc_auc_score(target_test, prediction_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Hyperparameter tuning\n",
    "The process of testing different values of hyperparameters to find the optimal ones: the one that gives the best predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross valudation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9853431  0.98533333 0.97466667 0.96466667 0.96       0.97933333\n",
      " 0.99       0.99333333 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Import the function for implementing cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use that function to print the cross validation score for 10 folds\n",
    "print(cross_val_score(model,features,target,cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch parameters\n",
    "Test different combinations of hyperparameters and run cross-validation on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values for maximum depth\n",
    "depth = [i for i in range(5,21,1)]\n",
    "\n",
    "# Generate values for minimum sample size\n",
    "samples = [i for i in range(50,500,50)]\n",
    "\n",
    "# Create the dictionary with parameters to be checked\n",
    "parameters = dict(max_depth=depth, min_samples_leaf=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'min_samples_leaf': 50}\n"
     ]
    }
   ],
   "source": [
    "# import the GridSearchCV function\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# set up parameters: done\n",
    "parameters = dict(max_depth=depth, min_samples_leaf=samples)\n",
    "\n",
    "# initialize the param_search function using the GridSearchCV function, initial model and parameters above\n",
    "param_search = GridSearchCV(model, parameters)\n",
    "\n",
    "# fit the param_search to the training dataset\n",
    "param_search.fit(features_train, target_train)\n",
    "\n",
    "# print the best parameters found\n",
    "print(param_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting important features\n",
    "We want to know which features have the strongest and weakest impacts on the decision te leave the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>satisfaction</th>\n",
       "      <td>0.499958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evaluation</th>\n",
       "      <td>0.153005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_spend_company</th>\n",
       "      <td>0.139567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_projects</th>\n",
       "      <td>0.098877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_montly_hours</th>\n",
       "      <td>0.088262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>0.006250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technical</th>\n",
       "      <td>0.003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>0.002248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>0.001549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandD</th>\n",
       "      <td>0.001415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_accident</th>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management</th>\n",
       "      <td>0.001194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IT</th>\n",
       "      <td>0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>0.000783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_mng</th>\n",
       "      <td>0.000576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promotion</th>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      importance\n",
       "satisfaction            0.499958\n",
       "evaluation              0.153005\n",
       "time_spend_company      0.139567\n",
       "number_of_projects      0.098877\n",
       "average_montly_hours    0.088262\n",
       "salary                  0.006250\n",
       "technical               0.003929\n",
       "support                 0.002248\n",
       "sales                   0.001549\n",
       "RandD                   0.001415\n",
       "work_accident           0.001316\n",
       "management              0.001194\n",
       "IT                      0.000925\n",
       "hr                      0.000783\n",
       "product_mng             0.000576\n",
       "marketing               0.000129\n",
       "promotion               0.000019"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a list of features: done\n",
    "feature_list = list(features)\n",
    "\n",
    "# Save the results inside a DataFrame using feature_list as an indnex\n",
    "relative_importances = pd.DataFrame(index=feature_list, data=feature_importances, columns=[\"importance\"])\n",
    "\n",
    "# Sort values to learn most important features\n",
    "relative_importances.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting important features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only features with relative importance higher than 1%\n",
    "selected_features = relative_importances[relative_importances.importance>0.01]\n",
    "\n",
    "# create a list from those features: done\n",
    "selected_list = selected_features.index\n",
    "\n",
    "# transform both features_train and features_test components to include only selected features\n",
    "features_train_selected = features_train[selected_list]\n",
    "features_test_selected = features_test[selected_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop and test the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.28\n",
      "91.75027870680044\n",
      "94.07002193314084\n"
     ]
    }
   ],
   "source": [
    "# Initialize the best model using parameters provided in description\n",
    "model_best = DecisionTreeClassifier(max_depth=8, min_samples_leaf=150, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Fit the model using only selected features from training set: done\n",
    "model_best.fit(features_train_selected, target_train)\n",
    "\n",
    "# Make prediction based on selected list of features from test set\n",
    "prediction_best = model_best.predict(features_test_selected)\n",
    "\n",
    "# Print the general accuracy of the model_best\n",
    "print(model_best.score(features_test_selected, target_test) * 100)\n",
    "\n",
    "# Print the recall score of the model predictions\n",
    "print(recall_score(target_test, prediction_best) * 100)\n",
    "\n",
    "# Print the ROC/AUC score of the model predictions\n",
    "print(roc_auc_score(target_test, prediction_best) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We have learned several things:\n",
    "- Building a Decision Tree\n",
    "- Evaluating the model\n",
    "- Tuning the model\n",
    "\n",
    "\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
